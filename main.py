import pandas as pd
from cve_feature_extractor import Cve_Feature_Extractor
import os
import consts
import datetime
import pandas as pd
from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
import consts
import os
import numpy
from gensim.models import Word2Vec
from nltk import word_tokenize
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import MinMaxScaler
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import ClusterCentroids
from imblearn.over_sampling import RandomOverSampler
from cwe_cluster_finder import cwe_cluster_finder
from nvd import NvdCve

def main():

    # Download NVD CVE JSON files from 2013 to 2022. Toggle comment if you have already downloaded them

    #download_cves_json()

    # extract vector features from json files and write them into a csv file. Toggle comment if they were already extracted

    CveFeatureExtractor = Cve_Feature_Extractor()
    #CveFeatureExtractor.feature_vectors_to_csv()

    # read csv into a pandas dataframe
    df_cves = pd.read_csv(consts.feature_matrix_url)

    # Models that we use
    ml_models = {}
    ml_models['decision_tree_classifier'] = DecisionTreeClassifier()
    ml_models['knn'] = KNeighborsClassifier()
    ml_models['logistic_regression'] = LogisticRegression()
    ml_models['random_forest'] = RandomForestClassifier()
    ml_models['svm'] = SVC()
    # Here I have tried to tune XGBClassifier a bit. It seems to get better overall performances using these hyperparameters
    ml_models['XGBClassifier'] = XGBClassifier(learning_rate=0.3, n_estimators=150, scale_pos_weight=1000, min_child_weight=3, gamma=1, subsample=1.0, colsample_bytree=1.0, max_depth=14)

    # Sklearn classes which help us address class imbalance
    smote = SMOTE(random_state=7)
    cc = ClusterCentroids(random_state=7)
    ros = RandomOverSampler(random_state=7)

    # Load feature matrix into a Pandas DataFrame which will be used to train our models
    df_cves = pd.read_csv(consts.feature_matrix_url)

    # Make sure that the DataFrame doesn't contain any broken rows
    df_cves = df_cves[df_cves['cve_id'].str.contains("CVE-") == True]
    df_cves = df_cves.dropna()

    # Load custom word embeddings if they exist, otherwise we calculate them again
    if not os.path.exists(consts.word_embeddings_path):
        train_and_save_word_embeddings_cves(df_cves)

    embeddings = Word2Vec.load(consts.word_embeddings_path)

    # This class is used to group CWES by primary cluster/secondary cluster
    cwe_cf = cwe_cluster_finder()
    #df_cves['cwe_value'] = df_cves['cwe_value'].map(lambda cwe : changeCweToPrimaryOrSecondaryCluster(cwe_cf, cwe, 'primary'))
    #df_cves['cwe_value'] = df_cves['cwe_value'].map(lambda cwe : changeCweToPrimaryOrSecondaryCluster(cwe_cf, cwe, 'sec'))

    # Label encoding
    labelEncoder = LabelEncoder()
    df_cves['cve_id'] = labelEncoder.fit_transform(df_cves['cve_id'])
    df_cves['cwe_value'] = labelEncoder.fit_transform(df_cves['cwe_value'])

    # Prepare word embeddings
    df_cves['clean_summary'] = df_cves['clean_summary'].map(lambda summary : summary.split())
    X_embeddings = df_cves['clean_summary']
    X_embeddings = X_embeddings.map(lambda summary : document_vector(summary, embeddings))
    X_embeddings = list(X_embeddings)

    # Prepare tabular data
    X_tabular = df_cves.drop(columns=["clean_summary", "class_exploited"],axis=1)

    # Features to predict
    y = df_cves["class_exploited"]
 
    # Some parameters to split our data sets into training and test sets
    test_size = 0.33
    seed = 7

    # Split tabular data into training and test set
    X_tabular_train, X_tabular_test, y_tabular_train, y_tabular_test =  train_test_split(X_tabular, y, test_size=test_size, random_state=seed, stratify=y)

    # normalize tabular data
    norm = MinMaxScaler().fit(X_tabular_train)
    X_tabular_train = norm.transform(X_tabular_train)
    X_tabular_test = norm.transform(X_tabular_test)

    # Class imbalance techniques
  
    # Apply SMOTE on tabular data
    #X_tabular_train, y_tabular_train = smote.fit_resample(X_tabular_train, y_tabular_train)

    # Apply random undersampling on tabular data
    #X_tabular_train, y_tabular_train = cc.fit_resample(X_tabular_train, y_tabular_train)

    # Apply random oversampling on tabular data
    #X_tabular_train, y_tabular_train = ros.fit_resample(X_tabular_train, y_tabular_train)

    # Split word embeddings into training and test set
    X_embedding_train, X_embedding_test, y_embedding_train, y_embedding_test =  train_test_split(X_embeddings, y, test_size=test_size, random_state=seed, stratify=y)
    
    # Apply SMOTE on word embeddings
    #X_embedding_train, y_embedding_train = smote.fit_resample(X_embedding_train, y_embedding_train)

    # Apply random undersampling on word embeddings
    #X_embedding_train, y_embedding_train = cc.fit_resample(X_embedding_train, y_embedding_train)

    # Apply random oversampling on word embeddings
    #X_embedding_train, y_embedding_train = ros.fit_resample(X_embedding_train, y_embedding_train)

    # Model selection & tuning

    tabular_sets = {
        'X_train':X_tabular_train,
        'X_test':X_tabular_test,
        'y_train':y_tabular_train,
        'y_test':y_tabular_test
    }

    embedding_sets = {
        'X_train':X_embedding_train,
        'X_test':X_embedding_test,
        'y_train':y_embedding_train,
        'y_test':y_embedding_test
    }

    # Train our models on tabular data and write down results
    train_models(ml_models, tabular_sets)

def tune_xgb(ml_models, training_set, metric="f1", n_splits=10, n_repeats=3, filename=consts.tuning_results_url):

    """
        Tune XGBoost. You can change this method and/or create new methods, which work like this one for tuning other models with different hyperparameters.
    """

    params = {
        'learning_rate': [0.03, 0.01, 0.003, 0.001],
        'min_child_weight': [1,3, 5,7, 10],
        'gamma': [0, 0.5, 1, 1.5, 2, 2.5, 5],
        'subsample': [0.6, 0.8, 1.0, 1.2, 1.4],
        'colsample_bytree': [0.6, 0.8, 1.0, 1.2, 1.4],
        'max_depth': [3, 4, 5, 6, 7, 8, 9 ,10, 12, 14],
        'reg_lambda':numpy.array([0.4, 0.6, 0.8, 1, 1.2, 1.4])
    }

    kfold = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=7)
    grid_search = GridSearchCV(ml_models['XGBClassifier'], params, scoring=metric, n_jobs=-1, cv=kfold)
    grid_result = grid_search.fit(training_set['X'], training_set['y'])

    # write results
    if os.path.exists(filename):
        os.remove(filename)

    with open(filename, 'w') as fd:
        fd.write("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
        fd.write("\n\n")
        means = grid_result.cv_results_['mean_test_score']
        stds = grid_result.cv_results_['std_test_score']
        params = grid_result.cv_results_['params']
        for mean, stdev, param in zip(means, stds, params):
            fd.write("%f (%f) with: %r" % (mean, stdev, param))
            fd.write("\n")
        
        fd.write("\n\n")

def train_models(ml_models, sets, type='tabular', filename=consts.model_selection_results_url):
    
    """
    Test all models on tabular data or word embeddings. Choose the type. If none specified, it will be tabular.
    Record the learning time, prediction time and evaluation time. 
    Write down the confusion matrix & classification report into a file. If no filename specified, it will be the one that was defined in consts.py.

    sets is a dict which contains the training set and the test set, split into X and y, depending on which data set you want to train the models on. 
    For example, to train tabular data, it should look like this:

    {
        'X_train':X_tabular_train
        'X_test':X_tabular_test
        'y_train':y_tabular_train
        'y_test':y_tabular_test
    }
    
    """

    if type == 'tabular':
        msg = 'Tabular data'
    elif type == 'we':
        msg = 'Word embeddings'
    else:
        raise Exception('You cannot train the models on something else than tabular data or word embeddings')
    
    # Overwrite the file which contains the results if it already exists. Be careful here!

    if os.path.exists(filename):
        os.remove(filename)

    with open(filename, 'w') as fd:

        for modelName, model in ml_models.items():
            fd.write(f"Model: {modelName}\n\n")
            fd.write(f"{msg}\n\n")
            temps1 = datetime.datetime.now()
            model.fit(sets['X_train'], sets['y_train'])
            temps2 = datetime.datetime.now()
            tempsdiff = temps2 - temps1

            fd.write(f"Learning time : {tempsdiff.total_seconds()} seconds.\n")

            temps1 = datetime.datetime.now()
            tab_y_pred = model.predict(sets['X_test'])
            temps2 = datetime.datetime.now()
            tempsdiff = temps2 - temps1
            fd.write(f"Prediction time : {tempsdiff.total_seconds()} seconds.\n")

            conf_matrix = metrics.confusion_matrix(sets['y_test'], tab_y_pred, labels=numpy.unique(tab_y_pred))
            for tab in conf_matrix:
                for val in tab:
                    fd.write(str(val))
                    fd.write(" ")
                fd.write("\n")
            fd.write("\n")
            fd.write(metrics.classification_report(sets['y_test'], tab_y_pred, labels=numpy.unique(tab_y_pred)))
            fd.write("\n\n")
    
def document_vector(doc, model):
    """Create document vectors by averaging word vectors. Remove out-of-vocabulary words."""
    doc = [word for word in doc if word in model.wv.index_to_key]
    return numpy.mean(model.wv[doc], axis=0)

def train_and_save_word_embeddings_cves(df_cves):

    tokenized_corpus = []
    
    for i in range(len(df_cves)):
        tokenized_corpus.append(word_tokenize(df_cves.iloc[i]['clean_summary']))

    model = Word2Vec(tokenized_corpus,min_count=1,vector_size=300)

    model.save(consts.word_embeddings_path)

def changeCweToPrimaryOrSecondaryCluster(cwe_cf, cwe, primOrSec='primary'):

    if primOrSec != 'primary' and primOrSec != 'sec':
        raise Exception("primOrSec param must be either primary or sec")

    cwe_number = ""

    if cwe.startswith('CWE-'):
        cwe_number = cwe[4:]

    cwe_clusters = cwe_cf.find_cluster(cwe_number)

    if cwe_clusters is None:
        return cwe
    
    if primOrSec == 'primary':
        return f"CWE-{cwe_clusters[0]}"
    
    if primOrSec == 'sec':
        if cwe_clusters[1] is None:
            return cwe
        else:
            return f"CWE-{cwe_clusters[1]}"

def download_cves_json():
    nvd_cve_downloader = NvdCve()

    # download json files
    nvd_cve_downloader.download_cve()


if __name__ == "__main__":
    main()